{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Classification By Phoneme Frequency\n",
    "Camille Girard\n",
    "\n",
    "Jonah Spicher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we would like to recognize the language of a word based on the phonemes in the word. Given a word, input in its phoneme form, and a predefined dataset of phoneme frequency in each language being tested, we will run through all the phonemes in the word and calculate the probability of each language. Combining all the phonemes of the word will return an updated probability that the observed word is in that language. \n",
    "\n",
    "Question: Let's say you are in a place where people are speaking five languages: English, Spanish, Italian, Mandarin, Arabic, and Hindi. You overhear someone say a word, based on the sounds in that word, which language are they speaking? \n",
    "Given each of these five words, (language, idioma,  语言, لغة, भाषा)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Usualy, it is fairly easy to tell what language someone is speaking just by hearing it, at least assuming you have heard that language spoken before. The information is being conveyed (obviously) by the sounds they are making, so it should be possible to guess the language of a word given the sounds that make that word up. Bayesian classification provides a fairly straightforward solution: figure out how often those phonemes appear in a given language, and the probability that you heard that language followes from Bayes theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Jupyter so figures appear in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure Jupyter to display the assigned value after an assignment\n",
    "%config InteractiveShell.ast_node_interactivity='last_expr_or_assign'\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from thinkbayes2 import Pmf, Cdf, Suite\n",
    "import thinkplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an even prior. This is obviously not a reasonable decision in real life. At the most basic level, you would want to factor in the percentage of people in the world who speak each language, but in reality your prior will skew heavily towards the domnant languages in your area. For this toy example in a mysterious room with only these five (somewhat random) languages, it seems like a reasonable choice, and will at least show how strong of evidence each word offers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic 0.16666666666666666\n",
      "English 0.16666666666666666\n",
      "Hindi 0.16666666666666666\n",
      "Italian 0.16666666666666666\n",
      "Mandarin 0.16666666666666666\n",
      "Spanish 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "languages = ['English', 'Spanish', 'Italian', 'Hindi','Arabic', 'Mandarin']\n",
    "lang_prior = Pmf(languages)\n",
    "lang_prior.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification code\n",
    "\n",
    "Next, we need to define our suite and its parameters. For now, a naive Bayesian model seems fine, but given time we will revisit this decision, as this ignores dipthongs, which often are defining features in a language. Here, though, our data is just a series of phonemes, and our hypothesis is a language. The likelihood, then, is just how often the given phoneme appears in our hypothetical language. We have stored these values in a .csv file, so likelihood is just a simple lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_csv('FILE NAME HERE', delimiter='\\t')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language(Suite):\n",
    "    \n",
    "    def Likelihood(self, data, hypo):\n",
    "        \"\"\"\n",
    "        data: string of phonemes (This should maybe be a list, because some phonemes may be more than one character)\n",
    "        hypo: name of languages\n",
    "        \"\"\"\n",
    "        language = hypo\n",
    "        like = 1\n",
    "        for i in data:\n",
    "            row = df.loc[df['Phonemes']==i].index\n",
    "            like *= table[language][row]\n",
    "        return like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Now, we just have to feed the Suite each word (in IPA) and it should get a decent idea of which language each word belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-357d8b67a243>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlang_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"leɪŋgwij\"\u001b[0m \u001b[0;31m# English\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlang_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlang_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinkbayes2/thinkbayes2.py\u001b[0m in \u001b[0;36mUpdate\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1402\u001b[0m         \"\"\"\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhypo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m             \u001b[0mlike\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-df9ad1a405be>\u001b[0m in \u001b[0;36mLikelihood\u001b[0;34m(self, data, hypo)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mlike\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Phonemes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mlike\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "lang_dist = Language(lang_prior)\n",
    "word = \"leɪŋgwij\" # English\n",
    "lang_dist.Update(word)\n",
    "lang_dist.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dist = Language(lang_prior)\n",
    "word = \"iðjoma\" # Spanish\n",
    "lang_dist.Update(word)\n",
    "lang_dist.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dist = Language(lang_prior)\n",
    "word = \"jujen\" # Mandarin maybe??\n",
    "lang_dist.Update(word)\n",
    "lang_dist.Print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dist = Language(lang_prior)\n",
    "word = \"word\" #Arabic will go here, once I figure that out\n",
    "lang_dist.Update(word)\n",
    "lang_dist.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like [it worked or it didnt], because feeding each word gave a [high or low] probability that the word belonged to the correct language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
